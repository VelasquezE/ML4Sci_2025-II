{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdWBdteiMhu+POAX9RshAv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VelasquezE/ML4Sci_2025-II/blob/main/Module2/clasficadorTexto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Clasificador de texto**"
      ],
      "metadata": {
        "id": "ttwJVT5PVcny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Importar las librerías necesarias"
      ],
      "metadata": {
        "id": "9SFE5YdgVm09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "import pickle\n",
        "from ipywidgets import FileUpload, VBox, Button, Output, Text\n",
        "import io"
      ],
      "metadata": {
        "id": "OS04hAOlV676"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Personalización gráficas\n",
        "\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "custom_rc = {\n",
        "    \"xtick.bottom\": True,\n",
        "    \"ytick.left\": True,\n",
        "    \"xtick.direction\": \"out\",\n",
        "    \"ytick.direction\": \"out\",\n",
        "    \"axes.edgecolor\": \"black\",\n",
        "    \"grid.color\": \"#bbbbbb\",\n",
        "    \"grid.linestyle\": \"dashed\",\n",
        "    \"axes.grid\": True,\n",
        "    \"axes.spines.top\": True,\n",
        "    \"axes.spines.right\": False,\n",
        "    \"axes.spines.left\": False,\n",
        "    \"axes.spines.bottom\": True\n",
        "}\n",
        "sns.set_context(\"notebook\")\n",
        "sns.set_style(\"darkgrid\", rc = custom_rc)"
      ],
      "metadata": {
        "id": "9kr3w1ZEg0C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Cargar los datos de entrenamiento"
      ],
      "metadata": {
        "id": "Lg1xBfM8XQK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se descargan los archivos csv de un repositorio de GitHub.\n",
        "Luego, se carga el contenido en un DataFrame de pandas para su posterior análisis."
      ],
      "metadata": {
        "id": "P3l9YkBfXeUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(name):\n",
        "    \"\"\"\n",
        "    Descarga un archivo csv dado el nombre del archivo.\n",
        "    Carga los datos en un DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Datos extraídos.\n",
        "    \"\"\"\n",
        "    base_url = \"https://raw.githubusercontent.com/VelasquezE/ML4Sci_2025-II/refs/heads/main/Module2/\"\n",
        "    url = base_url + name\n",
        "    output_file = name\n",
        "    !wget -O {output_file} {url}\n",
        "\n",
        "    data = pd.read_csv(output_file)\n",
        "    return data"
      ],
      "metadata": {
        "id": "CRAcrjmgXnjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"exc_03_train.csv\"\n",
        "\n",
        "data = get_data(file_name)"
      ],
      "metadata": {
        "id": "JqlJpnUiYIqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obsérvese la información básica de los datos,"
      ],
      "metadata": {
        "id": "J2_Rv4Gsz-0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "fjLNASwwaK5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "VEigA0EF149B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"Topic\"].value_counts()"
      ],
      "metadata": {
        "id": "YS17ilGw2RAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Pre-procesamiento de los datos"
      ],
      "metadata": {
        "id": "fIxb6qsMoxV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se empieza limpiando los datos. Se eliminan los caracteres especiales (números, puntuación, símbolos), se dejan las palabras en minúscula, la lematización convierte una palabra a su forma base (cars -> car), y quita las \"stopwords\" es decir \"the, a, an, in, on, with...\"."
      ],
      "metadata": {
        "id": "NtdDTsO6pPql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")"
      ],
      "metadata": {
        "id": "z8WTubF14ezc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text: str):\n",
        "  \"\"\"\n",
        "  Receives a text and deletes special caracters,\n",
        "  stop words, turns to lowercase, and applies\n",
        "  lemmatization.\n",
        "\n",
        "  Parameters:\n",
        "    text (str)\n",
        "\n",
        "  Returns:\n",
        "    Clean text.\n",
        "  \"\"\"\n",
        "  text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "  text = text.lower()\n",
        "  words = text.split()\n",
        "  words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]\n",
        "\n",
        "  return  \" \".join(words)\n",
        "\n",
        "\n",
        "def clean_text_batch(texts):\n",
        "  return [clean_text(t) for t in texts]"
      ],
      "metadata": {
        "id": "9CT9lkYqnGjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Crear pipeline"
      ],
      "metadata": {
        "id": "fHfyMvvK_LCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se crea un pipeline que realizará los siguientes pasos:\n",
        "1.   Aplicará la función creada para limpieza del texto\n",
        "2.   Vectoriza el texto en palabras\n",
        "3.   Aplica el modelo de clasificación\n",
        "\n"
      ],
      "metadata": {
        "id": "tiFkfTk9_j_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pipeline(classifier):\n",
        "  pipeline = Pipeline([\n",
        "      (\"cleaner\", FunctionTransformer(clean_text_batch)),\n",
        "      (\"tfidf\", TfidfVectorizer()),\n",
        "      (\"clf\", classifier)\n",
        "  ])\n",
        "\n",
        "  return pipeline"
      ],
      "metadata": {
        "id": "uZXEimQ2_xM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. División de los datos"
      ],
      "metadata": {
        "id": "XO-oK8bGBQk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se dividen los datos en un dataset de entrenamiento (80%) y otro de prueba (20%)."
      ],
      "metadata": {
        "id": "1bhTl2OhZ5k5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 19\n",
        "test_size = 0.2\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[\"Comment\"], data[\"Topic\"], test_size=test_size, random_state=seed)"
      ],
      "metadata": {
        "id": "HgMhBt1YaBjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Creación de los modelos"
      ],
      "metadata": {
        "id": "80t6UrSTBeC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se aplica `MultinomialNB()` y `ComplementNB()`."
      ],
      "metadata": {
        "id": "Xh2envhKBxXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_nb = create_pipeline(MultinomialNB())\n",
        "pipe_nbc = create_pipeline(ComplementNB())\n",
        "\n",
        "pipe_nb.fit(X_train, y_train)\n",
        "predict_nb = pipe_nb.predict(X_test)\n",
        "pipe_nbc.fit(X_train, y_train)\n",
        "predict_nbc = pipe_nbc.predict(X_test)"
      ],
      "metadata": {
        "id": "gPJINzDJBvrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Evaluación de los modelos"
      ],
      "metadata": {
        "id": "A0nO_maTDXTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se calcula la precisión,"
      ],
      "metadata": {
        "id": "4xJrHczCMoKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_nb = accuracy_score(y_test, predict_nb)\n",
        "accuracy_nbc = accuracy_score(y_test, predict_nbc)\n",
        "\n",
        "print(f'Accuracy MultinomialNB(): {(accuracy_nb * 100):.2f}%')\n",
        "print(f'Accuracy ComplementNB(): {(accuracy_nbc * 100):.2f}%')\n"
      ],
      "metadata": {
        "id": "LN8plAvoMchf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se grafica la matriz de confusión,"
      ],
      "metadata": {
        "id": "ArSyikPdM6r_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix_nb = confusion_matrix(y_test, predict_nb)\n",
        "conf_matrix_nbc = confusion_matrix(y_test, predict_nbc)\n",
        "\n",
        "class_labels = np.unique(y_train)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "\n",
        "sns.heatmap(conf_matrix_nb,\n",
        "            ax = ax1,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='crest',\n",
        "            cbar = False,\n",
        "            xticklabels=class_labels,\n",
        "            yticklabels=class_labels)\n",
        "sns.heatmap(conf_matrix_nbc,\n",
        "            ax = ax2,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='crest',\n",
        "            cbar = False,\n",
        "            xticklabels=class_labels,\n",
        "            yticklabels=class_labels)\n",
        "ax1.set_title('MultinomialNB')\n",
        "ax2.set_title('ComplementNB')\n",
        "fig.suptitle('Matriz de confusión')\n",
        "fig.supxlabel('Etiqueta predicha')\n",
        "fig.supylabel('Etiqueta verdadera')\n",
        "fig.savefig(\"confusion_matrix.jpg\", dpi = 400)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OwHVYQTQM89k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y, se aplica validación cruzada,"
      ],
      "metadata": {
        "id": "Q2oCO5MSOC3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "    (\"MultinomialNB\", MultinomialNB()),\n",
        "    (\"ComplementNB\", ComplementNB()),\n",
        "]\n",
        "\n",
        "for name, clf in models:\n",
        "    pipeline = create_pipeline(clf)\n",
        "    scores = cross_val_score(pipeline, data[\"Comment\"], data[\"Topic\"], cv=5, scoring='accuracy')\n",
        "    print(f\"{name} → Mean accuracy: {scores.mean():.4f}\")"
      ],
      "metadata": {
        "id": "HeSJnKEV2SUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Selección de modelo final"
      ],
      "metadata": {
        "id": "MIw6d86S9Oud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El método `ComplementNB()` fue el que realizó una mejor clasificación. Así, este se presentará como modelo final. Además, ya que se hicieron las pruebas, se entrena con todo el conjunto de datos."
      ],
      "metadata": {
        "id": "9sHb74J0OQlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_pipeline(ComplementNB())\n",
        "model.fit(data[\"Comment\"], data[\"Topic\"])"
      ],
      "metadata": {
        "id": "CY7xyUvVOn9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se guarda el modelo entrenado con `pickle`."
      ],
      "metadata": {
        "id": "1FJYgTmYysYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'text_classifier_model.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "uoRQdY7gyuzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se crea una función que permita al usuario ingresar nuevos mensajes:"
      ],
      "metadata": {
        "id": "ASYzUciy0rM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_category(text:str, model=model):\n",
        "  predicted_label = model.predict([text])\n",
        "\n",
        "  print(f\"The input text belongs to the '{predicted_label[0]}' category.\")"
      ],
      "metadata": {
        "id": "KTHCzBzuy_mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_message = \"I love spacecrafts\"\n",
        "predict_category(new_message, model)"
      ],
      "metadata": {
        "id": "fM0PBHeg0RBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_message = \"Hormones are crazy\"\n",
        "predict_category(new_message, model)"
      ],
      "metadata": {
        "id": "m1e3GAM61aTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "También, para que sea cómodo para el usuario, se crea una pequeña interfaz que solicite un nuevo conjunto de datos y calcule la matriz de confusión."
      ],
      "metadata": {
        "id": "Ke7nnZdd1pY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploader = FileUpload(accept='.csv', multiple=False)\n",
        "\n",
        "print(\"Por favor suba el archivo y escriba los nombres de las columnas para identificar\\n\"+\n",
        "      \"cuál corresponde a los comentarios y cuál a las etiquetas:\")\n",
        "\n",
        "text_column = Text(description=\"Columna texto: \",\n",
        "                        style = {'description_width': 'initial'})\n",
        "label_column = Text(description=\"Columna etiquetas: \",\n",
        "                         style = {'description_width': 'initial'})\n",
        "\n",
        "button = Button(description=\"Evaluar modelo\")\n",
        "out = Output()\n",
        "\n",
        "def evaluate_model(model):\n",
        "    out.clear_output()\n",
        "    with out:\n",
        "        if not uploader.value:\n",
        "            print(\"Por favor, suba un archivo CSV primero.\")\n",
        "            return\n",
        "\n",
        "        uploaded = list(uploader.value.values())[0]\n",
        "        data = pd.read_csv(io.BytesIO(uploaded['content']))\n",
        "        text_column_name = text_column.value.strip()\n",
        "        label_column_name = label_column.value.strip()\n",
        "\n",
        "        if text_column_name not in data.columns:\n",
        "            print(f\"La columna '{text_column_name}' no existe en el archivo.\")\n",
        "            return\n",
        "\n",
        "        if label_column_name not in data.columns:\n",
        "            print(f\"La columna '{label_column_name}' no existe en el archivo.\")\n",
        "            return\n",
        "\n",
        "        X_test = data[text_column_name]\n",
        "        y_true = data[label_column_name]\n",
        "\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        print(f'Accuracy: {(accuracy * 100):.2f}%')\n",
        "\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        class_labels = np.unique(y_true)\n",
        "\n",
        "        sns.heatmap(cm,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='crest',\n",
        "            cbar = False,\n",
        "            xticklabels=class_labels,\n",
        "            yticklabels=class_labels)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "button.on_click(lambda event: evaluate_model(model))\n",
        "\n",
        "display(VBox([uploader, text_column, label_column, button, out]))"
      ],
      "metadata": {
        "id": "-kSBiTxr1dON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "64AmlP_P5GZs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}